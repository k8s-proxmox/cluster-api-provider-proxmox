apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: "${CLUSTER_NAME}"
  namespace: "${NAMESPACE}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: ProxmoxCluster
    name: "${CLUSTER_NAME}"
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: "${CLUSTER_NAME}"
  controlPlaneEndpoint:
    host: "${CONTROLPLANE_HOST}"
    port: 6443

---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ProxmoxCluster
metadata:
  name: "${CLUSTER_NAME}"
  namespace: "${NAMESPACE}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  controlPlaneEndpoint:
    host: "${CONTROLPLANE_HOST}"
    port: 6443
  serverRef:
    endpoint: "${PROXMOX_URL}"
    secretRef:
      name: "${CLUSTER_NAME}"
  storage:
    name: "${CLUSTER_NAME}"
    path: ""

---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: "${CLUSTER_NAME}"
  namespace: "${NAMESPACE}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-provider: external
      controllerManager:
        extraArgs:
          cloud-provider: external
      networking:
        dnsDomain: cluster.local
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
    files:
      - content: |
          apiVersion: v1
          kind: Pod
          metadata:
            creationTimestamp: null
            name: kube-vip
            namespace: kube-system
          spec:
            containers:
            - args:
              - manager
              env:
              - name: cp_enable
                value: "true"
              - name: vip_interface
                value: ${VIP_NETWORK_INTERFACE=""}
              - name: address
                value: ${CONTROLPLANE_HOST}
              - name: port
                value: "6443"
              - name: vip_arp
                value: "true"
              - name: vip_leaderelection
                value: "true"
              - name: vip_leaseduration
                value: "15"
              - name: vip_renewdeadline
                value: "10"
              - name: vip_retryperiod
                value: "2"
              image: ghcr.io/kube-vip/kube-vip:v0.5.11
              imagePullPolicy: IfNotPresent
              name: kube-vip
              resources: {}
              securityContext:
                capabilities:
                  add:
                  - NET_ADMIN
                  - NET_RAW
              volumeMounts:
              - mountPath: /etc/kubernetes/admin.conf
                name: kubeconfig
            hostAliases:
            - hostnames:
              - kubernetes
              ip: 127.0.0.1
            hostNetwork: true
            volumes:
            - hostPath:
                path: /etc/kubernetes/admin.conf
                type: FileOrCreate
              name: kubeconfig
          status: {}
        owner: root:root
        path: /etc/kubernetes/manifests/kube-vip.yaml
    postKubeadmCommands:
      - "curl -L https://dl.k8s.io/release/v1.27.3/bin/linux/amd64/kubectl -o /usr/local/bin/kubectl"
      - "chmod +x /usr/local/bin/kubectl"
      - "reboot now"
    preKubeadmCommands: []
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: ProxmoxMachineTemplate
      name: ${CLUSTER_NAME}-controlplane
  replicas: ${CONTROL_PLANE_MACHINE_COUNT:=3}
  version: ${KUBERNETES_VERSION:=v1.27.3}

---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ProxmoxMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-controlplane
  namespace: "${NAMESPACE}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  template:
    spec:
      image:
        url: https://cloud-images.ubuntu.com/releases/jammy/release-20230914/ubuntu-22.04-server-cloudimg-amd64-disk-kvm.img
        checksum: c5eed826009c9f671bc5f7c9d5d63861aa2afe91aeff1c0d3a4cb5b28b2e35d6
        checksumType: sha256
      hardware:
        cpu: 4
        memory: 8192
      cloudInit:
        user:
          packages:
            - socat
            - conntrack
          writeFiles:
            - path: /etc/modules-load.d/k8s.conf
              owner: root:root
              permissions: "0640"
              content: overlay\nbr_netfilter
            - path: /etc/sysctl.d/k8s.conf
              owner: root:root
              permissions: "0640"
              content: |
                net.bridge.bridge-nf-call-iptables  = 1
                net.bridge.bridge-nf-call-ip6tables = 1
                net.ipv4.ip_forward                 = 1
          runCmd:
            - "modprobe overlay"
            - "modprobe br_netfilter"
            - "sysctl --system"
            - "mkdir -p /usr/local/bin"
            - curl -L "https://github.com/containerd/containerd/releases/download/v1.7.2/containerd-1.7.2-linux-amd64.tar.gz" | tar Cxvz "/usr/local"
            - curl -L "https://raw.githubusercontent.com/containerd/containerd/main/containerd.service" -o /etc/systemd/system/containerd.service
            - "mkdir -p /etc/containerd"
            - "containerd config default > /etc/containerd/config.toml"
            - "sed 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml -i"
            - "systemctl daemon-reload"
            - "systemctl enable --now containerd"
            - "mkdir -p /usr/local/sbin"
            - curl -L "https://github.com/opencontainers/runc/releases/download/v1.1.7/runc.amd64" -o /usr/local/sbin/runc
            - "chmod 755 /usr/local/sbin/runc"
            - "mkdir -p /opt/cni/bin"
            - curl -L "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz" | tar -C "/opt/cni/bin" -xz
            - curl -L "https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.27.0/crictl-v1.27.0-linux-amd64.tar.gz" | tar -C "/usr/local/bin" -xz
            - curl -L --remote-name-all https://dl.k8s.io/release/${KUBERNETES_VERSION:=v1.27.3}/bin/linux/amd64/kubeadm -o /usr/local/bin/kubeadm
            - chmod +x /usr/local/bin/kubeadm
            - curl -L --remote-name-all https://dl.k8s.io/release/${KUBERNETES_VERSION:=v1.27.3}/bin/linux/amd64/kubelet -o /usr/local/bin/kubelet
            - chmod +x /usr/local/bin/kubelet
            - curl -sSL "https://raw.githubusercontent.com/kubernetes/release/v0.15.1/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service" | sed "s:/usr/bin:/usr/local/bin:g" | tee /etc/systemd/system/kubelet.service
            - mkdir -p /etc/systemd/system/kubelet.service.d
            - curl -sSL "https://raw.githubusercontent.com/kubernetes/release/v0.15.1/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf" | sed "s:/usr/bin:/usr/local/bin:g" | tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
            - "systemctl enable kubelet.service"

---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: ${NAMESPACE}
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  clusterName: "${CLUSTER_NAME}"
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels: {}
  template:
    spec:
      clusterName: ${CLUSTER_NAME}
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-md-0
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: ProxmoxMachineTemplate
        name: ${CLUSTER_NAME}-md-0
      version: ${KUBERNETES_VERSION:=v1.27.3}

---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: ${NAMESPACE}
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external

---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ProxmoxMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: ${NAMESPACE}
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  template:
    spec:
      image:
        url: https://cloud-images.ubuntu.com/releases/jammy/release-20230914/ubuntu-22.04-server-cloudimg-amd64-disk-kvm.img
        checksum: c5eed826009c9f671bc5f7c9d5d63861aa2afe91aeff1c0d3a4cb5b28b2e35d6
        checksumType: sha256
      cloudInit:
        user:
          packages:
            - socat
            - conntrack
          writeFiles:
            - path: /etc/modules-load.d/k8s.conf
              owner: root:root
              permissions: "0640"
              content: overlay\nbr_netfilter
            - path: /etc/sysctl.d/k8s.conf
              owner: root:root
              permissions: "0640"
              content: |
                net.bridge.bridge-nf-call-iptables  = 1
                net.bridge.bridge-nf-call-ip6tables = 1
                net.ipv4.ip_forward                 = 1
          runCmd:
            - "modprobe overlay"
            - "modprobe br_netfilter"
            - "sysctl --system"
            - "mkdir -p /usr/local/bin"
            - curl -L "https://github.com/containerd/containerd/releases/download/v1.7.2/containerd-1.7.2-linux-amd64.tar.gz" | tar Cxvz "/usr/local"
            - curl -L "https://raw.githubusercontent.com/containerd/containerd/main/containerd.service" -o /etc/systemd/system/containerd.service
            - "mkdir -p /etc/containerd"
            - "containerd config default > /etc/containerd/config.toml"
            - "sed 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml -i"
            - "systemctl daemon-reload"
            - "systemctl enable --now containerd"
            - "mkdir -p /usr/local/sbin"
            - curl -L "https://github.com/opencontainers/runc/releases/download/v1.1.7/runc.amd64" -o /usr/local/sbin/runc
            - "chmod 755 /usr/local/sbin/runc"
            - "mkdir -p /opt/cni/bin"
            - curl -L "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz" | tar -C "/opt/cni/bin" -xz
            - curl -L "https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.27.0/crictl-v1.27.0-linux-amd64.tar.gz" | tar -C "/usr/local/bin" -xz
            - curl -L --remote-name-all https://dl.k8s.io/release/${KUBERNETES_VERSION:=v1.27.3}/bin/linux/amd64/kubeadm -o /usr/local/bin/kubeadm
            - chmod +x /usr/local/bin/kubeadm
            - curl -L --remote-name-all https://dl.k8s.io/release/${KUBERNETES_VERSION:=v1.27.3}/bin/linux/amd64/kubelet -o /usr/local/bin/kubelet
            - chmod +x /usr/local/bin/kubelet
            - curl -sSL "https://raw.githubusercontent.com/kubernetes/release/v0.15.1/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service" | sed "s:/usr/bin:/usr/local/bin:g" | tee /etc/systemd/system/kubelet.service
            - mkdir -p /etc/systemd/system/kubelet.service.d
            - curl -sSL "https://raw.githubusercontent.com/kubernetes/release/v0.15.1/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf" | sed "s:/usr/bin:/usr/local/bin:g" | tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
            - "systemctl enable kubelet.service"

---
apiVersion: v1
stringData:
  PROXMOX_PASSWORD: ${PROXMOX_PASSWORD:=""}
  PROXMOX_USER: ${PROXMOX_USER:=""}
  PROXMOX_TOKENID: ${PROXMOX_TOKENID:=""}
  PROXMOX_SECRET: ${PROXMOX_SECRET:=""}
kind: Secret
metadata:
  name: "${CLUSTER_NAME}"
  namespace: "${NAMESPACE}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
type: Opaque

---
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  name: ${CLUSTER_NAME}-crs-0
  namespace: "${NAMESPACE}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  clusterSelector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
  resources:
    - kind: ConfigMap
      name: cloud-controller-manager
    - kind: ConfigMap
      name: flannel-cni
  strategy: Reconcile

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cloud-controller-manager
  namespace: "${NAMESPACE}"
data:
  cloud-controller-manager.yaml: |
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: proxmox-cloud-controller-manager
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: system:proxmox-cloud-controller-manager
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
    - kind: ServiceAccount
      name: proxmox-cloud-controller-manager
      namespace: kube-system
    ---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      labels:
        k8s-app: cloud-controller-manager
      name: cloud-controller-manager
      namespace: kube-system
    spec:
      selector:
        matchLabels:
          k8s-app: cloud-controller-manager
      template:
        metadata:
          labels:
            k8s-app: cloud-controller-manager
        spec:
          serviceAccountName: proxmox-cloud-controller-manager
          containers:
          - name: cloud-controller-manager
            image: ghcr.io/k8s-proxmox/cloud-provider-proxmox:latest
            command:
            - /usr/local/bin/cloud-controller-manager
            - --cloud-provider=proxmox
            - --cloud-config=/etc/proxmox/config.yaml
            - --leader-elect=true
            - --use-service-account-credentials
            - --controllers=cloud-node,cloud-node-lifecycle
            volumeMounts:
              - name: cloud-config
                mountPath: /etc/proxmox
                readOnly: true
            livenessProbe:
              httpGet:
                path: /healthz
                port: 10258
                scheme: HTTPS
              initialDelaySeconds: 20
              periodSeconds: 30
              timeoutSeconds: 5
          volumes:
            - name: cloud-config
              secret:
                secretName: cloud-config
          tolerations:
          - key: node.cloudprovider.kubernetes.io/uninitialized
            value: "true"
            effect: NoSchedule
          - key: node-role.kubernetes.io/control-plane
            operator: Exists
            effect: NoSchedule
          - key: node-role.kubernetes.io/master
            operator: Exists
            effect: NoSchedule
          nodeSelector:
            node-role.kubernetes.io/control-plane: ""
    ---
    apiVersion: v1
    kind: Secret
    metadata:
      name: cloud-config
      namespace: kube-system
    stringData:
      config.yaml: |
        proxmox:
          url: ${PROXMOX_URL}
          user: ${PROXMOX_USER:=""}
          password: ${PROXMOX_PASSWORD:=""}
          tokenID: ${PROXMOX_TOKENID:=""}
          secret: ${PROXMOX_SECRET:=""}

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: flannel-cni
  namespace: ${NAMESPACE}
data:
  flannel-cni.yaml: |
    apiVersion: v1
    kind: Namespace
    metadata:
      labels:
        k8s-app: flannel
        pod-security.kubernetes.io/enforce: privileged
      name: kube-flannel
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      labels:
        k8s-app: flannel
      name: flannel
      namespace: kube-flannel
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      labels:
        k8s-app: flannel
      name: flannel
    rules:
    - apiGroups:
      - ""
      resources:
      - pods
      verbs:
      - get
    - apiGroups:
      - ""
      resources:
      - nodes
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - nodes/status
      verbs:
      - patch
    - apiGroups:
      - networking.k8s.io
      resources:
      - clustercidrs
      verbs:
      - list
      - watch
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      labels:
        k8s-app: flannel
      name: flannel
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: flannel
    subjects:
    - kind: ServiceAccount
      name: flannel
      namespace: kube-flannel
    ---
    apiVersion: v1
    data:
      cni-conf.json: |
        {
          "name": "cbr0",
          "cniVersion": "0.3.1",
          "plugins": [
            {
              "type": "flannel",
              "delegate": {
                "hairpinMode": true,
                "isDefaultGateway": true
              }
            },
            {
              "type": "portmap",
              "capabilities": {
                "portMappings": true
              }
            }
          ]
        }
      net-conf.json: |
        {
          "Network": "10.244.0.0/16",
          "Backend": {
            "Type": "vxlan"
          }
        }
    kind: ConfigMap
    metadata:
      labels:
        app: flannel
        k8s-app: flannel
        tier: node
      name: kube-flannel-cfg
      namespace: kube-flannel
    ---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      labels:
        app: flannel
        k8s-app: flannel
        tier: node
      name: kube-flannel-ds
      namespace: kube-flannel
    spec:
      selector:
        matchLabels:
          app: flannel
          k8s-app: flannel
      template:
        metadata:
          labels:
            app: flannel
            k8s-app: flannel
            tier: node
        spec:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: kubernetes.io/os
                    operator: In
                    values:
                    - linux
          containers:
          - args:
            - --ip-masq
            - --kube-subnet-mgr
            command:
            - /opt/bin/flanneld
            env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: EVENT_QUEUE_DEPTH
              value: "5000"
            image: docker.io/flannel/flannel:v0.23.0
            name: kube-flannel
            resources:
              requests:
                cpu: 100m
                memory: 50Mi
            securityContext:
              capabilities:
                add:
                - NET_ADMIN
                - NET_RAW
              privileged: false
            volumeMounts:
            - mountPath: /run/flannel
              name: run
            - mountPath: /etc/kube-flannel/
              name: flannel-cfg
            - mountPath: /run/xtables.lock
              name: xtables-lock
          hostNetwork: true
          initContainers:
          - args:
            - -f
            - /flannel
            - /opt/cni/bin/flannel
            command:
            - cp
            image: docker.io/flannel/flannel-cni-plugin:v1.2.0
            name: install-cni-plugin
            volumeMounts:
            - mountPath: /opt/cni/bin
              name: cni-plugin
          - args:
            - -f
            - /etc/kube-flannel/cni-conf.json
            - /etc/cni/net.d/10-flannel.conflist
            command:
            - cp
            image: docker.io/flannel/flannel:v0.23.0
            name: install-cni
            volumeMounts:
            - mountPath: /etc/cni/net.d
              name: cni
            - mountPath: /etc/kube-flannel/
              name: flannel-cfg
          priorityClassName: system-node-critical
          serviceAccountName: flannel
          tolerations:
          - effect: NoSchedule
            operator: Exists
          volumes:
          - hostPath:
              path: /run/flannel
            name: run
          - hostPath:
              path: /opt/cni/bin
            name: cni-plugin
          - hostPath:
              path: /etc/cni/net.d
            name: cni
          - configMap:
              name: kube-flannel-cfg
            name: flannel-cfg
          - hostPath:
              path: /run/xtables.lock
              type: FileOrCreate
            name: xtables-lock
